
#### 分布式数据存储需求
1. scaleable: 扩展能力（当台节点负载是有限的，希望将系统分散到其他的节点.
2. tolerant + highAvailbel: 容错率和高可用性(当一组机器里面某一个机器当机以后，我们还希望它可以提供正常的服务) 
3. latency延迟: 多个数据中心,提高终端用户体验性

#### 扩展能力
&emsp;一般扩展横向扩展和纵向扩展，这两种扩展对应了三种架构(这三种架构各有优劣):
1. 基于共享内存架构
> 缺点:成本的增加超过线性的(2倍的资源，成本可能不止2倍，但是性能未必能达到两倍)，硬件资源达到一定情况下，每增加一点，它的成本可能会增加好几倍
> 优点:但是它有不错的容错能力（现在基本上磁盘都支持热插拔）一块磁盘坏了可以立马替换
2. 基于共享存储架构
> 优点:横线扩展，增加机器，然后数据放到可共享的磁盘阵列上面(独立的CPU，独立的内存，高速的网络IO)，
> 缺点:但是受限于分布式锁限制（一般数据仓库这么干）
3. 无共享架构---分布式存储
> 优点:独立的CPU，独立的内存，独立的存储，每个存储软件都是一个单独的节点，节点的通信建立在以太网之上，数据同步等核心的逻辑依靠软件来实现。 不需要专门的硬件（大部分核心的逻辑都是软件来实现的
> 缺点: 但是给应用程序带来了很高的复杂性

#### 复制和分区：
&emsp;无论是哪一种架构,将数据分布在不同节点一般有以下两种方式
(但是大部分分布式存储都是两者集合使用)
- 复制：多节点保存相同的数据，复制提供冗余---- replicastion ---复制用来提供数据的可用性
- 分区：将一个数据拆分多个小的子集存储在不同的节点 ----- shards -----分片用来提升数据访问的性能


## 0x01 复制
&emsp;复制只要是为了解决如下的问题:
1. 使数据在地里位置上更接近用户(spark里面也有类似的机制,会加载离自己更近的分区)
2. 提供高可用性(当一个节点当机以后，数据仍然可以使用)
3. 多台机器同时提供访问(提高吞吐量)一台节点的io是有限的

&emsp;如果数据一层不变，直接copy完事，但是一般情况下数据都是会实时变化的
复制一般主要有以下三种场景（几乎所有的分布式复制都是用了下面的一种)
1. 主从辅助
2. 多主节点复制
3. 无主节点复制

&emsp;复制过程中可能还需要处理很多细节，
1. 同步复制还是异步复制
2. 失败的副本如何处理
3. 最终一致性的问题，如何解决复制滞后问题


### 0x00 主节点和从节点下的复制
&emsp;主从拓扑下的复制逻辑可以归纳为下面几个步骤:
1. 客户端请求master节点
2. master节点将日志写入本地
3. master节点将日志同步的从节点 {
	1. 等接收到从节点确认信息，在通知客户端------同步模式 {
			保证数据一致性
			但是可用性会降低,延迟也是有问题
	}
	2. master节点不等待从节点确认信息，就直接通知客户端---异步模式{
			数据可能不一致
			但是可用性会很高
	}
	3. 指定个数从节点确认完毕,通知客户端------半同步模式 ---折中的方法
}

#### 0x00 从节点加入集群
&emsp; 从节点如何加入集群(参考raft的日志同步)
1. 某个时间点产生快照
2. 快照copy到从节点
3. 从节点连接到主节点
4. 从快照日志序列那个地方开始同步日志--然后重复1-4

#### 0x01 节点失败
&emsp; 节点失败一般分为两类,从节点失败,主节点失败:
- 从节点失败：
> 从节点可以根据当前的日志，继续从主节点开始同步日志，直到追赶上主节点
- 主节点失败
	1. 将某个从节点提升为主节点(例如nodeA)
	2. 客户端的主节点地址更新为nodeA
	3. 其他节点开始同步nodeA 的数据
	切换的过程可能是手动或者程序自动，但是切换逻辑一般如下:
	1. 确认主节点失败（可能有个仲裁节点健康检测等等其他机制)
	2. 选举新节点（人工，或者仲裁节点 或者分布式选举),选择主的时候应该选择数据最接近原主的那个节点(但是未必)或者其他机制
	3. 重新配置系统，使其生效
切换过程中变数
	1. 如果是异步复制,新节点没有收到原maser节点全部数据,但是原master又很快的恢复了并且加入集群
	   导致问题: 原master可能未意识到自己已经不是master了，并且还向其他节点同步数据,新主节点可能收到冲突的写请求, 解决办法：原主节点上还没有复制的请求直接丢弃,但是可能会违背数据持久化更新
	2. 和其他系统依赖问题
	3. 脑裂问题
	4. 检测机制,频繁的切换会导致很严重的性能问题

#### 0x02 日志复制实现
1. 基于语句的复制
	原理:  主节点执行的每个语句,都当作一条日志复制给从节点,从节点会分析执行这些sql语句(就好像他们来自客户端的一样)
	问题:  {
		1. 非确定函数可能会在不同的副本里面的值不一致(随机数/时间等等)
		2. 如果数据库有自增列或者依赖现有数据的,必须严格按照顺序执行的,在高并发下会有严重的性能问题
		3. 副作用语句(1也可以当作副作用语句),在每个节点产生的效果不一样
	}
	解决: 非确定语句直接替换固定的值,但是也有很多边界条件限制(应该有限考虑其他的方法)
2. 基于预写日志复制
	对数据库写入的字节序列先追加到日志里面
	1. 完全可以使用日志在另外一个节点构建一份新副本
	2. 也可以通过网络将wal日志复制给从节点,从节点可以构建和主节点一样的副本
3. 基于行的逻辑日志复制
    这种复制和存储逻辑解耦，忽略具体的语句，以实际的存储数据来代表日志
    例如:
       行加入: 这个行包含了当前行所有相关列的新值
    这种逻辑日志和存储解耦可以做到版本之间的兼容性
4. 基于触发器的复制
   基于某些条件复制

#### 0x02复制滞后问题
&emsp;日志 异步情况下，复制滞后会导致数据不一致,这个取决于用户对最终一致性的取舍,一般会出现滞后的场景:
&emsp;读自己的写
- 造成问题原因: 写master，读未同步的从，造成读写不一致，如何解决写后读一致
- 解决问题思路：从主读可能会被修改的数据，从备读不回被修改的数据（但是如何却分是否被修改又是另外一个话题）
&emsp;单调读 ---单调读一致性
- 造成问题原因: 写master 第一次读同步复制从节点1，获取到结果 第二次读异步复制从节点2(此时还没复制完成), 获取不到结果
- 解决问题方案：需要确保用户总是从一个固定的节点/副本读取数据
&emsp; 前缀一致读（happend-before问题）
- 造成问题原因: 写master log1, log2 log1 经历10s 到达从节点， log2 经历2s到达从节点，造成写入顺序的错乱
- 解决问题方案: 需要有方法来判断先后顺序



## 0x02 多节点复制
&emsp;主从节点复制问题受限于一个主节点,主节点之间网络异常回造成整个写的异常,多主模型
适用场景：多数据中心
优点:
	1. 性能:n终端用户更好（智能dns会把用户请求路由到最近的数据中心)
	2. 容忍数据中心失效(每个数据中心都可以当作独立的一个数据中心，独立于另外一个数据中心运行)
	3. 容忍网络问题(主从同步操作，数据中心之间异步，临时网路中断不会影响数据最终一致性)
问题:
	由于多个主都可以接受写请求，因此需要解决写冲突的问题（这个其实很难避免，也很复杂，尽可能避免写冲突问题)
解决冲突:
	为每个写分配一个id,选择ID较高的为胜利者{
		失败者--丢弃----造成数据丢失
		合并: 可能会有数据混乱
		记录: 让用户来处理
	}

## 0x03 无主节点复制
&emsp;和主从复制/主主复制不一样， 无主节点复制是客户端会直接将请求写入请求发送到多个副本

### 失效节点时写入数据库
&emsp;问题: 写入日志时,有一个节点离线,导致节点数据不一致
&emsp;解决: 这时候就需要通过一些手段来解决,例如定义一个版本来确定那个值是最新的(或者时间戳, 但是时间也是不可信)

### 解决数据收敛方法分为两类:
- 读修复
> 客户端读取数据，根据版本来判断哪个是最新的，再把最新的数据给update到旧的节点里面
- 反熵
> 程序后台查找数据的不一致，将新数据从一个副本copy到另外一个副本，但是无法保证顺序，所以会有前缀一致读的问题


### `quorum` 对读写一致性的影响
&emsp;客户对最终一致性的要求可以通过quorum的宽松程度来配置(主题思想和半异步复制基本类似)
&emsp; 读写quorum要求:一般情况下共识w(写节点个数) + r(读节点个数) > n(副本), 那么读取的节点一定会包含最新的值, 其实 quorum并没有严格限制，但是关键quorum关键在于读写节点里面一定要有一个重叠的节点


### 无主节点复制的多数据中心下的情况
&emsp;这个和多主复制没啥区别，主要还是解决冲突的问题,一般可以通过如下手段:
检测并发写 
1. 最后写入者获胜
&emsp;这种实现收敛的方法：每个副本总是保存最新的值,允许覆盖丢弃旧值(因此需要找到一个合适的方法来判断哪个是新值哪个是旧值就可以了,例如 LWW算法或者采用UUID )
2. happen-before和并发关系
&emsp;判断并发关键在于确定是否happen-before,如果是并发就需要解决冲突问题
3. 确定前后关系
4. 合并同时写入的值
&emsp;上面一些算法用来确保数据不会丢失， 但是需要用户来合并写请求
5. 版本矢量
