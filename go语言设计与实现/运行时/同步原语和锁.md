### mutex的本质
&emsp;`mutex`本质上是一个状态机,借助`CSP`来更新锁的状态,协作式调度阻塞当前的`g`,通过自旋来霸占CPU,通过信号量来唤醒`g`

### `Mutex`
&emsp;锁的状态及其状态转换如下:
![概况]()
&emsp;`Mutex`由两个字段来表示 ,state用来表示当前互斥锁的状态, 而sema用于控制锁状态的信号量,加起来总共占8个字节(0代表锁处于未上锁的状态)
```go
// A Mutex is a mutual exclusion lock.
// The zero value for a Mutex is an unlocked mutex.
//
// A Mutex must not be copied after first use.
type Mutex struct {
	state int32
	sema  uint32
}
```
&emsp;互斥锁的状态如下
&emsp;默认情况下互斥锁的状态位0,int32中不同的位分别表示了不同的状态:
- `mutexLocked`:  表示互斥锁的锁定状态
- `mutexToken`:     表示互斥锁从正常模式被唤醒了
- `mutexStarving`       表示当前的互斥锁进入饥饿状态
- `waitersCount`    当前互斥锁上等待的gorotine数量

&emsp;`mutext`有两种状态(这两种状态在一定条件下可以切换):
- 正常模式: 锁的等待遵循先进先出的顺序(但是新加入的goroutine要比等待唤醒的goroutine又具有优势,因为他们已经在CPU上运行了,这个会导致等待的队列一直无法获取到锁,这种情况下新加入的goroutine会被放到队列的头部)
- 饥饿模式: 饥饿模式下锁会直接从未锁定的goroutine转移到等待队列的头部goroutine.在饥饿模式下，新的g不会去获取锁的所有权/也不会自旋,他们会直接放到队列的尾部

#### 加锁和解锁
&emsp; 加锁和解锁分别使用了`sync.Mutex.Lock`和`sync.Mutex.UnLock`方法. 互斥量加锁是靠`sync.Mutex.Lock`来完成的, 当互斥锁的状态为0的时候,`sync.Mutex.Lock`会把`mutexLocked`设置为1， 思维导图如下:
![加锁]()

&emsp;加锁一般分为三种情况:
- 没有冲突(也就是锁处于没有被锁定的状态): 直接把当前的锁设置为加锁状态;
- 有冲突, 开始自旋,等待锁释放,如果其他的goroutine在该时间段里面释放了该锁,直接获得该锁,如果没有进入第三种情况
- 有冲突, 而且已经过了自旋阶段,然后调用semrelease让goroutine进入等待状态
```go 
func (m *Mutex) Lock() {
	// Fast path: grab unlocked mutex.
    // 当当前互斥锁的状态为0的时候, 将mutexLocked的值设置为1
	if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
		if race.Enabled {
			race.Acquire(unsafe.Pointer(m))
		}
		return
	}
	// Slow path (outlined so that the fast path can be inlined)
	m.lockSlow()
}
```
&emsp;如果当前互斥锁的状态不是0,那么则调用m.lockSlow()方法.`m.lockSlow`方法如下,主要工作流程有如下几个步骤:
1. 判断当前的goroutine能不能进入自旋
2. 通过自旋等待互斥锁释放
3. 计算互斥锁的最新状态
4. 更新互斥锁的状态并且获取锁

```go 
// 自旋case， case1
func (m *Mutex) lockSlow() {
	for {
        // 如果旧的锁状态是锁定状态(非饥饿状态)，并且 它可以进入自旋模式
		if old&(mutexLocked|mutexStarving) == mutexLocked && runtime_canSpin(iter) {
			if !awoke && old&mutexWoken == 0 && old>>mutexWaiterShift != 0 &&
				atomic.CompareAndSwapInt32(&m.state, old, old|mutexWoken) {
				awoke = true
			}
			runtime_doSpin() // 进入自旋模式
			iter++
			old = m.state
			continue
            // 一直等待到它可以通过自旋转释放锁
		}
	}
}
// Active spinning for sync.Mutex.
//go:linkname sync_runtime_canSpin sync.runtime_canSpin
//go:nosplit
func sync_runtime_canSpin(i int) bool {
    //如果自旋次数大于4 或者CPU个数小于=1,或者 不可以陷入自旋
	if i >= active_spin || ncpu <= 1 || gomaxprocs <= int32(sched.npidle+sched.nmspinning)+1 {
		return false
	}
    // 当前至少存在一个正在运行的处理器，并且处理器的队列为空
	if p := getg().m.p.ptr(); !runqempty(p) {
		return false
	}
	return true
}

//go:linkname sync_runtime_doSpin sync.runtime_doSpin
//go:nosplit
func sync_runtime_doSpin() {
	procyield(active_spin_cnt)
}

// x86上procyield的代码
TEXT runtime·procyield(SB),NOSPLIT,$0-0
	MOVL	cycles+0(FP), AX
again:
	PAUSE
	SUBL	$1, AX
	JNZ	again
	RET
```
&emsp;自旋锁是一种多线程同步机制,当前进程进入自旋过程中会一直保持CPU的占用,持续检查某个条件是不是为真.在多个CPU上,自旋可以避免goroutine的切换,使用恰当会对性能有很大的提升,(但是使用不当会拖慢整个程序,所以goroutine进入自旋的条件比较苛刻):
1. 只有普通模式下的锁才允许进入自旋模式
2. 看`sync_runtime_canSpin` 注释

&emsp;一旦当前goroutine能够进入自旋就会调用`runtime.sync_runtime_doSpin`和`runtime.procyield`并且执行30次的PAUSE指令(PAUSE只会占用CPU并且消耗CPU时间)

#### 自旋完成后，会根据上下文的状态计算出互斥锁的最新的状态
```go 
func (m *Mutex) lockSlow() {
    for {
        ... 等待自旋
        // 自旋转完成，更新锁的状态
		new := old
		// Don't try to acquire starving mutex, new arriving goroutines must queue.
        // 如果当前处于正常模式下, 将锁的状态设置为锁定状态(如果是处于饥饿模式下，新的G需要排队才能获取到锁)
		if old&mutexStarving == 0 { new |= mutexLocked }
        //  如果当前处于锁定状态或者饥饿状态，那么new state的值 + 8
		if old&(mutexLocked|mutexStarving) != 0 { new += 1 << mutexWaiterShift } 
		// The current goroutine switches mutex to starvation mode.
		// But if the mutex is currently unlocked, don't do the switch.
		// Unlock expects that starving mutex has waiters, which will not
		// be true in this case.
		if starving && old&mutexLocked != 0 { new |= mutexStarving }
		if awoke {
			// The goroutine has been woken from sleep,
			// so we need to reset the flag in either case.
			if new&mutexWoken == 0 { throw("sync: inconsistent mutex state") }
			new &^= mutexWoken
		}
        // 计算出锁的状态后，使用CAS函数来更新更新锁的状态
        if atomic.CompareAndSwapInt32(&m.state, old, new) { // 通过CSA函数来获取锁
            if old&(mutexLocked|mutexStarving) == 0 {
				break // 通过 CAS 函数获取了锁
			}
            // 会不断的尝试获取锁,然后陷入休眠,直到获取到锁.
            runtime_SemacquireMutex(&m.sema, queueLifo, 1)
			if old&mutexStarving != 0 { // 原始状态为饥饿状态
                break
            }

        } else {
            // 更新状态
        }
    } 
}

//go:linkname sync_runtime_SemacquireMutex sync.runtime_SemacquireMutex
func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int) {
	semacquire1(addr, lifo, semaBlockProfile|semaMutexProfile, skipframes)
}
func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int) {
	gp := getg()
	if gp != gp.m.curg {
		throw("semacquire not on the G stack")
	}

	// Easy case.
	if cansemacquire(addr) {
		return
	}

	// Harder case:
	//	increment waiter count
	//	try cansemacquire one more time, return if succeeded
	//	enqueue itself as a waiter
	//	sleep
	//	(waiter descriptor is dequeued by signaler)
	s := acquireSudog()
	root := semroot(addr)
	t0 := int64(0)
	s.releasetime = 0
	s.acquiretime = 0
	s.ticket = 0
	if profile&semaBlockProfile != 0 && blockprofilerate > 0 {
		t0 = cputicks()
		s.releasetime = -1
	}
	if profile&semaMutexProfile != 0 && mutexprofilerate > 0 {
		if t0 == 0 {
			t0 = cputicks()
		}
		s.acquiretime = t0
	}
	for {
		lockWithRank(&root.lock, lockRankRoot)
		// Add ourselves to nwait to disable "easy case" in semrelease.
		atomic.Xadd(&root.nwait, 1)
		// Check cansemacquire to avoid missed wakeup.
		if cansemacquire(addr) {
			atomic.Xadd(&root.nwait, -1)
			unlock(&root.lock)
			break
		}
		// Any semrelease after the cansemacquire knows we're waiting
		// (we set nwait above), so go to sleep.
		root.queue(addr, s, lifo)
		goparkunlock(&root.lock, waitReasonSemacquire, traceEvGoBlockSync, 4+skipframes)
		if s.ticket != 0 || cansemacquire(addr) {
			break
		}
	}
	if s.releasetime > 0 {
		blockevent(s.releasetime-t0, 3+skipframes)
	}
	releaseSudog(s)
}
```


### Unlock
&emsp;解锁过程如下:
![解锁]()

```go 
func (m *Mutex) Unlock() {
	if race.Enabled {
		_ = m.state
		race.Release(unsafe.Pointer(m))
	}

	// Fast path: drop lock bit.
	new := atomic.AddInt32(&m.state, -mutexLocked)
	if new != 0 {
		// Outlined slow path to allow inlining the fast path.
		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
		m.unlockSlow(new)
	}
}

func (m *Mutex) unlockSlow(new int32) {
    // 验证锁的合法性
	if (new+mutexLocked)&mutexLocked == 0 {
		throw("sync: unlock of unlocked mutex")
	}
	if new&mutexStarving == 0 {     // 正常模式下的锁
		old := new
		for {
            // 如果没有等待着,或者一个goroutine已经被唤醒获得了锁或者，没有必要唤醒其他认了
			// If there are no waiters or a goroutine has already been woken or grabbed the lock, no need to wake anyone.
            // 在饥饿模式下
			// In starvation mode ownership is directly handed off from unlocking
			// goroutine to the next waiter. We are not part of this chain,
			// since we did not observe mutexStarving when we unlocked the mutex above.
			// So get off the way.
            // 没有等待着 或者 锁当前处于未锁定的状态 === 退出
			if old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken|mutexStarving) != 0 {
				return
			}
			// Grab the right to wake someone.
			new = (old - 1<<mutexWaiterShift) | mutexWoken
			if atomic.CompareAndSwapInt32(&m.state, old, new) {
				runtime_Semrelease(&m.sema, false, 1)
				return
			}
			old = m.state
		}
	} else { 
        // 饥饿模式下可以立马将所有权转移给下一个等待着,然后迭代一个时间切片，以便下一个waiter可以立马执行
		// Starving mode: handoff mutex ownership to the next waiter, and yield our time slice so that the next waiter can start to run immediately.
        //  mutexLocked没有被设置, waiter将会在被唤醒后立马设置它
		// Note: mutexLocked is not set, the waiter will set it after wakeup.
        // 但是锁如果饥饿模式被设置了,那么它仍然被认为是锁定的状态, 接下来的goroutine 也无法获取到它
		// But mutex is still considered locked if mutexStarving is set, so new coming goroutines won't acquire it.
		runtime_Semrelease(&m.sema, true, 1)
	}
}
```
